{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook structure\n",
    "  ## - 1. build a two layers CNN\n",
    "  ## - 2. Add more layers to build a deep CNN\n",
    "    - 3 convolution layers\n",
    "    - 2 fully connected layers\n",
    "###          -  compare this two CNN structure\n",
    "  ## - 3.`keep_prob` rate VS `dropout` rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. build a two layers CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.358494761\n",
      "Epoch: 0002 cost = 0.097940919\n",
      "Epoch: 0003 cost = 0.070362408\n",
      "Epoch: 0004 cost = 0.056949024\n",
      "Epoch: 0005 cost = 0.047772202\n",
      "Epoch: 0006 cost = 0.039817694\n",
      "Epoch: 0007 cost = 0.034906245\n",
      "Epoch: 0008 cost = 0.030535142\n",
      "Epoch: 0009 cost = 0.027481223\n",
      "Epoch: 0010 cost = 0.023806905\n",
      "Epoch: 0011 cost = 0.021045868\n",
      "Epoch: 0012 cost = 0.018816113\n",
      "Epoch: 0013 cost = 0.015678865\n",
      "Epoch: 0014 cost = 0.014327407\n",
      "Epoch: 0015 cost = 0.011730368\n",
      "Learning Finished!\n",
      "Accuracy: 0.9849\n",
      "Label:  [3]\n",
      "Prediction:  [3]\n"
     ]
    }
   ],
   "source": [
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "\n",
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiCo/wP6ORdRkn0onQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a91fe69e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - compare this two CNN structure\n",
    "### 2 layers CNN\n",
    "\n",
    "- layer1  L1 ImgIn shape=(?, 28, 28, 1)     \n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)     \n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)     \n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)     \n",
    "\n",
    "- layer2  L2 ImgIn shape=(?, 14, 14, 32)     \n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)      \n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)      \n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)     \n",
    "\n",
    "- Final FC 7x7x64 inputs -> 10 outputs     \n",
    "Tensor(\"***Reshape_1:0***\", shape=(?, 3136), dtype=float32)     \n",
    "\n",
    "\n",
    "**- Epoch: 0015 cost = 0.011730368**\n",
    "- Learning Finished!\n",
    "- **Accuracy: 0.9849**\n",
    "\n",
    "========================================================================\n",
    "\n",
    "**- Epoch: 0015 cost = 0.025256706**\n",
    "- Learning Finished!\n",
    "- **Accuracy: 0.9932**\n",
    "  \n",
    "\n",
    "### 5 layers(DEEP) CNN\n",
    "\n",
    "\n",
    "- layer1  L1 ImgIn shape=(?, 28, 28, 1)     \n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)    \n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)    \n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)    \n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)     \n",
    " \n",
    "- layer2  L2 ImgIn shape=(?, 14, 14, 32)     \n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)    \n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)    \n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)     \n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)    \n",
    "\n",
    "- layer3  L3 ImgIn shape=(?, 7, 7, 64)    \n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)    \n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)    \n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)    \n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)    \n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)    \n",
    "\n",
    "- layer4  L4 FC 4x4x128 inputs -> 625 outputs    \n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)     \n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)    \n",
    "\n",
    "- layer5 L5 Final FC 625 inputs -> 10 outputs    \n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. `keep_prob` rate VS `dropout` rate\n",
    "In TensorFlow, there are two functions that perform the dropout\n",
    "\n",
    "`tf.layers.dropout` uses `dropout` rate\n",
    " - 0.7 means 70% will be dropped\n",
    "\n",
    "`tf.contrib.layers.dropout` uses `keep_prob` instead\n",
    " - 0.7 means 70% will be kept\n",
    "\n",
    "**`dropout rate = 1 - keep_prob`**\n",
    "\n",
    "### \"`dropout`\" is usually inserted before the `FC layers` \n",
    "### it's not standard to insert it before the Conv-Layers.\n",
    "\n",
    "The reason is that Conv-Layers already have local sparse connection structures \n",
    "and are less prone to overfitting.\n",
    "\n",
    "Nevertheless, it's okay to use it in the scripts for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DEEP CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.410797768\n",
      "Epoch: 0002 cost = 0.099555811\n",
      "Epoch: 0003 cost = 0.075134781\n",
      "Epoch: 0004 cost = 0.062321061\n",
      "Epoch: 0005 cost = 0.052901570\n",
      "Epoch: 0006 cost = 0.048923441\n",
      "Epoch: 0007 cost = 0.042905460\n",
      "Epoch: 0008 cost = 0.038277411\n",
      "Epoch: 0009 cost = 0.035422171\n",
      "Epoch: 0010 cost = 0.035207811\n",
      "Epoch: 0011 cost = 0.031567930\n",
      "Epoch: 0012 cost = 0.028431041\n",
      "Epoch: 0013 cost = 0.029581306\n",
      "Epoch: 0014 cost = 0.027195767\n",
      "Epoch: 0015 cost = 0.025256706\n",
      "Learning Finished!\n",
      "Accuracy: 0.9932\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIQfCIrwA0ERfiAowg8ERfiBoP4HG/9KAFuCCUIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe7175e6e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''\n",
    "\n",
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "\n",
    "# if you have a OOM error, please refer to lab-11-X-mnist_deep_cnn_low_memory.py\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].\n",
    "          reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
